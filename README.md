# EvalAgent-A-Multi-Agent-Autonomous-Code-Evaluation-Hiring-Framework
EvalAgent is a multi-agent AI framework that autonomously evaluates software engineering submissions using Docker sandbox execution, UI automation, and LLM-based rubric scoring with debate arbitration. It replaces manual technical review with structured, reproducible, and bias-aware evaluation.

## ğŸš€ Why This Project?

Traditional technical evaluations suffer from:

- Manual code inspection
- Inconsistent scoring
- Subjective bias
- Limited runtime validation
- No structured rubric enforcement

EvalAgent solves this by combining:

- ğŸ³ Docker sandbox execution
- ğŸŒ Playwright-based UI automation
- ğŸ§  Multi-agent LLM debate scoring
- ğŸ“Š Structured rubric enforcement
- ğŸ“ Automated Excel reporting

---

## ğŸ— Architecture Overview

The system follows a modular multi-agent architecture:
Candidate Repo
â†“
Executor Agent (Docker Sandbox)
â†“
Static + Feature Analysis
â†“
UI Automation Agent (Playwright)
â†“
Judge A (Strict)
Judge B (Pragmatic)
â†“
Final Arbitration Judge
â†“
Aggregator (Scale Enforcement)
â†“
Excel Report

The system will:
1. Execute projects in sandbox
2. Evaluate features
3. Perform UI testing (if enabled)
4. Run multi-agent LLM scoring
5. Generate Excel report


## **ğŸ” Design Philosophy**
EvalAgent separates:
Judgement (LLMs)
Governance (Aggregator enforcement)
Execution (Docker sandbox)
Validation (Feature/UI agents)

This ensures robustness, fairness, and reproducibility.



## ğŸ§  Agents in the System

### 1ï¸âƒ£ Executor Agent
- Runs candidate project inside Docker
- Installs dependencies
- Captures logs and runtime failures
- Prevents host contamination

### 2ï¸âƒ£ Static & Feature Agent
- Extracts project structure
- Parses README
- Infers implemented features
- Detects missing core files

### 3ï¸âƒ£ UI Agent (Optional)
- Launches browser using Playwright
- Captures screenshots
- Tests responsiveness
- Detects frontend failures

### 4ï¸âƒ£ Multi-Agent Debate Scoring
Two LLM judges independently score:

- Problem Understanding (20)
- Architecture & Code Quality (30)
- Feature Completeness (30)
- UI/UX (20)

A final arbitration agent resolves scoring differences.

### 5ï¸âƒ£ Aggregator Layer
- Enforces rubric scale constraints
- Clamps invalid scores
- Normalizes outputs
- Formats final structured report

---

## ğŸ“Š Output Format

Each candidate is evaluated as:

| Candidate Name | Problem (20) | Architecture (30) | Feature (30) | UI/UX (20) | Total (100) | Final (/10) | Remarks |
|---------------|--------------|-------------------|--------------|------------|-------------|-------------|---------|

Excel file: `evaluation_results.xlsx`

---
## **Author**
Namrata Nair
AI Engineer | Agent Systems | LLM Architect
Focused on autonomous systems, evaluation pipelines, and multimodal AI frameworks.

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone <repo>
cd evalagent

python3 -m venv venv
source venv/bin/activate

pip install pandas openpyxl playwright ollama
playwright install

**## Install Docker**
Make sure Docker is installed and running.

Place candidate folders inside:
candidates/

python run_agentic_evaluation.py

